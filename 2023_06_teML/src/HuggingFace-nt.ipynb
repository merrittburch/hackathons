{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832185ea-4e3f-45a7-a136-42c9f03fc0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b765c930-357a-4aa6-bf15-1e1084edc9dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"InstaDeepAI/nucleotide-transformer-2.5b-multi-species\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"InstaDeepAI/nucleotide-transformer-2.5b-multi-species\")\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfefd25d-119c-4eca-b2cb-0af03332d044",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSeq = pd.read_csv(filepath_or_buffer='/workdir/jz963/0.data_from_James/Pg_X_test.txt', delimiter='\\t')\n",
    "trainSeq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f00288-1f01-4178-bd9e-f59d6c8ab2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSeq.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebe940d-af0c-4053-a5b2-bad6469fb731",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputSeq = trainSeq['upSeq2k']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9855d77-2ade-4781-8e77-d542593910f9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "resEmbeddings = np.empty((0, 2560))\n",
    "for i in range(0, len(inputSeq), 10):\n",
    "    tokens_ids = tokenizer.batch_encode_plus(inputSeq[i:i+10], return_tensors=\"pt\", \n",
    "                                             padding=True, truncation=True)[\"input_ids\"]\n",
    "    tokens_ids = tokens_ids.to(device)\n",
    "    attention_mask = tokens_ids != tokenizer.pad_token_id\n",
    "    with torch.no_grad():\n",
    "        torch_outs = model(\n",
    "        tokens_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        encoder_attention_mask=attention_mask,\n",
    "        output_hidden_states=True)\n",
    "    embeddings = torch_outs['hidden_states'][-1].detach().cpu().numpy()\n",
    "    attention_mask = tokens_ids != tokenizer.pad_token_id\n",
    "    attention_mask = attention_mask.cpu().numpy()\n",
    "    attention_mask = np.expand_dims(attention_mask, axis = -1)\n",
    "    masked_embeddings = embeddings * attention_mask  # multiply by 0 pad tokens embeddings\n",
    "    sequences_lengths = np.sum(attention_mask, axis=1)\n",
    "    mean_embeddings = np.sum(masked_embeddings, axis=1) / sequences_lengths\n",
    "    resEmbeddings = np.append(resEmbeddings, mean_embeddings, axis=0)\n",
    "    # print progress bar\n",
    "    print(f\"{i+1}/{len(inputSeq)}\", end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8681d2c7-0af4-432b-92a1-84214b5cd9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(fname = '/workdir/mbb262/Pg_X_test.txt', X=resEmbeddings, delimiter='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugging-face",
   "language": "python",
   "name": "hugging-face"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
